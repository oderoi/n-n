
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../Documentation/documentation/">
      
      
      <link rel="icon" href="../assets/16.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.47">
    
    
      
        <title>Progress_and_Roadmap - nan</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.6f8fc17f.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#home-page-documentation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="nan" class="md-header__button md-logo" aria-label="nan" data-md-component="logo">
      
  <img src="../assets/16.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            nan
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Progress_and_Roadmap
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="nan" class="md-nav__button md-logo" aria-label="nan" data-md-component="logo">
      
  <img src="../assets/16.svg" alt="logo">

    </a>
    nan
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Progress_and_Roadmap
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Progress_and_Roadmap
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#home-page-documentation" class="md-nav__link">
    <span class="md-ellipsis">
      Home Page | Documentation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-components" class="md-nav__link">
    <span class="md-ellipsis">
      Core Components
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Core Components">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#loss-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizers" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#layers" class="md-nav__link">
    <span class="md-ellipsis">
      Layers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-milestones" class="md-nav__link">
    <span class="md-ellipsis">
      Future Milestones
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Documentation/documentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documentation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Documentation/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#home-page-documentation" class="md-nav__link">
    <span class="md-ellipsis">
      Home Page | Documentation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-components" class="md-nav__link">
    <span class="md-ellipsis">
      Core Components
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Core Components">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#loss-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizers" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#layers" class="md-nav__link">
    <span class="md-ellipsis">
      Layers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-milestones" class="md-nav__link">
    <span class="md-ellipsis">
      Future Milestones
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<!-- <h1 align='center'><b>nano</b></h1> -->

<div align="center">

<picture>
  <source media="(prefers-color-scheme: light)" srcset="assets/nan.svg">
  <img alt="nan corp logo" src="assets/16.svg" width="100%" height="100%">
</picture>

</div>

<p><strong>nan</strong>: Something between <a href="https://tinygrad.org/">tinygrad</a>, <a href="https://github.com/pytorch/pytorch">PyTorch</a>, <a href="https://github.com/karpathy/micrograd">karpathy/micrograd</a>, <a href="https://gitlab.epfl.ch/hugon/pytorch/-/tree/master/aten/src">Aten</a> and <a href="https://openxla.org/xla">XLA</a>. Maintained by <a href="https://github.com/oderoi/nanoTorch/tree/main">nano corp</a>.</p>
<h3 id="home-page-documentation"><a href="../"><strong>Home Page</strong></a> | <a href="../Documentation/documentation/"><strong>Documentation</strong></a></h3>
<p><a href="https://github.com/oderoi/nanoTorch/stargazers"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/oderoi/nanoTorch" /></a></p>
<hr />
<h1 align='center'><b>Progress and Roadmap</b></h1>

<p>Nano is designed to provide an accessible, low-level deep learning framework with a focus on simplicity and modularity. Here’s a roadmap showcasing its primary components and future progress milestones:</p>
<h2 id="core-components">Core Components</h2>
<p>❌: Not implemented<br />
✅: Done</p>
<ol>
<li>
<p>Tensor Operations</p>
</li>
<li>
<p>Tensor Creation and Manipulation: Support for tensor creation with various data types (float, double, int) and shapes.</p>
</li>
</ol>
<table>
<thead>
<tr>
<th>Task</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tensor</td>
<td>✅</td>
</tr>
</tbody>
</table>
<ul>
<li>Basic Tensor Math:</li>
</ul>
<!-- | Task       | Status |
|------------|--------|
| ADD        |   ✅   |
| SUB        |   ✅   |
| MUL        |   ✅   |
| DIV        |   ✅   |
| MATMUL     |   ✅   |
| EXP        |   ✅   |
| LOG        |   ❌   |
| POW        |   ✅   |
| SUM        |   ✅   |
| TRANSPOSE  |   ❌   |
| FLATTEN    |   ❌   |
| RESHAPE    |   ❌   |
| CONV2D     |   ❌   |
| CONV3D     |   ❌   |
| MAXPOOL2D  |   ❌   |
| MAXPOOL3D  |   ❌   | -->

<table>
<thead>
<tr>
<th>Operation</th>
<th>Formula</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Addition</td>
<td><span class="arithmatex">\(C_{i,j} = A_{i,j} + B_{i,j}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Subtraction</td>
<td><span class="arithmatex">\(C_{i,j} = A_{i,j} - B_{i,j}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Maltiplication</td>
<td><span class="arithmatex">\(C_{i,j} = A_{i,j} * B_{i,j}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Division</td>
<td><span class="arithmatex">\(C_{i,j} = A_{i,j} / B_{i,j}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Dot_Product</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(C_{i,j} = \sum_{k=0}^{k-1} \left(A_{i,k} \cdot B_{k,j}\right)\)</span>\)</span></td>
<td></td>
</tr>
<tr>
<td>✅</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Exponent</td>
<td><span class="arithmatex">\(C_{i,j} = e^{x_{i,j}}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Logarithm</td>
<td><span class="arithmatex">\(C_{i,j} = \log_{10}(X_{i,j})\)</span></td>
<td>❌</td>
</tr>
<tr>
<td>Power</td>
<td><span class="arithmatex">\(C_{i,j} = (\mathbf{A}^p){i,j} = (\mathbf{A}{i,j})^n\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Sum</td>
<td><span class="arithmatex">\(\mathbf{C}   = \sum_{i=0}^{i-1}\(X_{i}\)\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Transpose</td>
<td><span class="arithmatex">\((\mathbf{A}^\top){i,j} = (\mathbf{A}){j,i}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Flatten</td>
<td><span class="arithmatex">\(\text{Flatten}(A_{m,n}) = \[A_{0,0},\  A_{0,1}, \dots\,\  A_{m-1, n-1}]\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Reshape</td>
<td><span class="arithmatex">\(\text{Reshape}(A_{m,n}) = A_{n,m}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Identity matrix (eye)</td>
<td><span class="arithmatex">\(\text{The Identity matrix }{I_n}\text{of size}{n} {x} {n}\text{is defined as:} \ \ {I_{ij}} = \bigg( \frac{1 \text{if} i = j}{0 \text{if} i\ne j}\)</span></td>
<td>✅</td>
</tr>
</tbody>
</table>
<p>Where:
- <span class="arithmatex">\(\(A_{ij}\)\)</span>, <span class="arithmatex">\(\(B_{ij}\)\)</span>, and <span class="arithmatex">\(\(C_{ij}\)\)</span> represent elements at the <span class="arithmatex">\(\(i\)-th\)</span> row and <span class="arithmatex">\(\(j\)-th\)</span> column of matrices <span class="arithmatex">\(\(A\)\)</span>, <span class="arithmatex">\(\(B\)\)</span>, and <span class="arithmatex">\(\(C\)\)</span>, respectively.
- The matrices <span class="arithmatex">\(\(A\)\)</span> and <span class="arithmatex">\(\(B\)\)</span> must have the same dimensions for addition to be valid.</p>
<ul>
<li>Operation Derivative</li>
</ul>
<table>
<thead>
<tr>
<th>Operation Derivative</th>
<th>Formula</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Addition_backward</td>
<td><span class="arithmatex">\(\frac{\partial C}{\partial A} = I, \quad \frac{\partial C}{\partial B} = I\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Subtraction_backward</td>
<td><span class="arithmatex">\(\frac{\partial C}{\partial A} = I, \quad \frac{\partial C}{\partial B} = -I\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Maltiplication_backward</td>
<td><span class="arithmatex">\(\frac{\partial C}{\partial A} = B, \quad \frac{\partial C}{\partial B} = A\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Division_backward</td>
<td><span class="arithmatex">\(\frac{\partial C}{\partial A} = B, \quad \frac{\partial C}{\partial B} = A\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Dot_Product_backward</td>
<td><span class="arithmatex">\(\frac{\partial C}{\partial A} = I, \quad \frac{\partial C}{\partial B} = I\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Exponent_backward</td>
<td><span class="arithmatex">\(\frac{\partial C}{\partial X} = e^{x_{i,j}}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Logarithm_backward</td>
<td><span class="arithmatex">\(\frac{\partial C}{\partial X} = \frac{1}{X}\)</span></td>
<td>❌</td>
</tr>
<tr>
<td>Power_backward</td>
<td><span class="arithmatex">\(\frac{\partial C}{\partial A} = B \cdot A^{n-1}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Sum_backward</td>
<td><span class="arithmatex">\(\frac{\partial C}{\partial X_i} = 1\ \  \text{for each}\ \  {i}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>Transpose_backward</td>
<td>Not applicable for individual elements but preserves structure.</td>
<td>-</td>
</tr>
<tr>
<td>Flatten_backward</td>
<td>No derivative directly, but a 1-to-1 mapping between elements is maintained.</td>
<td>-</td>
</tr>
<tr>
<td>Reshape_backward</td>
<td>No direct derivative as it doesn’t involve computation. Used for data structure organization.</td>
<td>-</td>
</tr>
</tbody>
</table>
<ul>
<li>Memory Management: Efficient use of malloc, calloc, memcpy, and memset for optimized memory handling.</li>
</ul>
<table>
<thead>
<tr>
<th>Task</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Free Tensor</td>
<td>✅</td>
</tr>
</tbody>
</table>
<ol>
<li>
<p>Automatic Differentiation</p>
</li>
<li>
<p>Gradient Storage: Each tensor can store its gradient, initialized with calloc for zeroing the memory.</p>
</li>
</ol>
<table>
<thead>
<tr>
<th>Task</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>grad</td>
<td>✅</td>
</tr>
<tr>
<td>requires_grad</td>
<td>✅</td>
</tr>
</tbody>
</table>
<ul>
<li>Backward Propagation: Simple backpropagation framework to calculate gradients for model parameters.</li>
</ul>
<table>
<thead>
<tr>
<th>Task</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Backward</td>
<td>✅</td>
</tr>
</tbody>
</table>
<ul>
<li>Operators for Gradient Tracking: Support for chaining operations to compute gradients through layers of the network.</li>
</ul>
<table>
<thead>
<tr>
<th>Task</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>prev</td>
<td>✅</td>
</tr>
<tr>
<td>op</td>
<td>✅</td>
</tr>
<tr>
<td>num_prev</td>
<td>✅</td>
</tr>
</tbody>
</table>
<ol>
<li>
<p>Basic Neural Network Layers</p>
</li>
<li>
<p>Linear (Dense) Layer: Implement a fully connected layer, allowing the network to learn transformations.</p>
</li>
</ol>
<table>
<thead>
<tr>
<th>Task</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear</td>
<td>❌</td>
</tr>
</tbody>
</table>
<ul>
<li>Activation Functions: Include foundational activation functions (e.g., ReLU, Sigmoid, Tanh) with support for gradient calculations.</li>
</ul>
<p>i.  Activations</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Formular</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReLU</td>
<td><span class="arithmatex">\(\text{ReLU({x})} = \bigg(\frac{ {x}\ \text{if} {x}  \geq\ 0} {0  \text{if} {x} &lt; 0}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>sigmoid</td>
<td><span class="arithmatex">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>tanh</td>
<td><span class="arithmatex">\(\text{tanh}(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>softmax</td>
<td><span class="arithmatex">\(\text{Sofmax}{(x_i)} = \frac{e^{x_i - \text{max(x)}}}{\sum_{j}{e^{x_j - \text{max(x)}}}}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>LeakyReLU</td>
<td><span class="arithmatex">\(\text{LeakyReLU({x})} = \bigg( \frac{ {x }\ \text{ if } {x }  \geq\ 0} {\alpha {x}  \text{ if } {x } &lt; 0}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>mean</td>
<td><span class="arithmatex">\(\mu = \frac{1}{n} \sum_{i=1}^n x_i\)</span></td>
<td>✅</td>
</tr>
</tbody>
</table>
<ul>
<li>Note: Softmax Numerical Stability<ul>
<li>When  x  has large values,  <span class="arithmatex">\(e^{x_i}\)</span>  may overflow. For numerical stability, PyTorch internally subtracts the maximum value from  <span class="arithmatex">\(x\)</span>  before applying the softmax:</li>
</ul>
</li>
</ul>
<p>ii. Activations Derivative</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Formular</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReLU_backward</td>
<td><span class="arithmatex">\(\frac{\partial}{\partial{x}} = \bigg( \frac{ {1 }\ \text{ if } {x }  \geq\ 0} {0  \text{ if } {x } \le 0}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>sigmoid_backward</td>
<td><span class="arithmatex">\(\sigma{\prime}(x) = \sigma(x)(1 - \sigma(x))\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>tanh_backward</td>
<td><span class="arithmatex">\(\text{tanh}{\prime}(x) = 1 - \text{tanh}^2(x)\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>softmax_backward</td>
<td><span class="arithmatex">\(\frac{\partial}{\partial{x_k}} = \text{Softmax}{(x_k)}(1 - \text{Softmax}{(x_k)})_{(diagonal: )} cross-element\ requires\ Jacobian\)</span></td>
<td>❌</td>
</tr>
<tr>
<td>LeakyReLU_backward</td>
<td><span class="arithmatex">\(\frac{\partial}{\partial{x}} = \bigg( \frac{ {1 }\ \text{ if } {x }  \geq\ 0} {\alpha  \text{ if } {x } \le 0}\)</span></td>
<td>✅</td>
</tr>
<tr>
<td>mean_backward</td>
<td><span class="arithmatex">\(\frac{\partial{\mu}}{\partial{x_i}} = \frac{1}{n}\)</span></td>
<td>✅</td>
</tr>
</tbody>
</table>
<ul>
<li>Note: Derivative of Softmat.<ul>
<li>The derivative depends on whether you’re computing it for the same index <span class="arithmatex">\(( i = j )\)</span> or different indices <span class="arithmatex">\(( i \neq j )\)</span>.</li>
<li>For a vector  <span class="arithmatex">\(\mathbf{s} = \text{Softmax}(\mathbf{x})\)</span> , the derivative is a matrix (Jacobian) given by:</li>
<li>When  <span class="arithmatex">\(i = j\)</span> : The derivative is  <span class="arithmatex">\(s_i (1 - s_i)\)</span> , representing the change in  <span class="arithmatex">\(s_i\)</span>  with respect to  <span class="arithmatex">\(x_i\)</span> .</li>
<li>When  <span class="arithmatex">\(i \neq j\)</span> : The derivative is  <span class="arithmatex">\(-s_i s_j\)</span> , showing the interaction between different outputs of the softmax.</li>
</ul>
</li>
</ul>
<div class="arithmatex">\[\frac{\partial s_i}{\partial x_j} =
\begin{cases}
s_i (1 - s_i), &amp; \text{if } i = j \\
-s_i s_j, &amp; \text{if } i \neq j
\end{cases}\]</div>
<ul>
<li>Matrix Form of the Derivative (Jacobian).<ul>
<li>The derivative can be represented as a Jacobian matrix for the softmax vector:</li>
</ul>
</li>
</ul>
<div class="arithmatex">\[\mathbf{J}(\mathbf{s}) = \text{diag}(\mathbf{s}) - \mathbf{s} \mathbf{s}^T\]</div>
<ul>
<li>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(\text{diag}(\mathbf{s})\)</span> : Diagonal matrix with  <span class="arithmatex">\(s_i\)</span>  on the diagonal.</li>
<li>
<p><span class="arithmatex">\(\mathbf{s} \mathbf{s}^T\)</span> : Outer product of  <span class="arithmatex">\(\mathbf{s}\)</span>  with itself.</p>
</li>
<li>
<p>Explicitly:</p>
</li>
</ul>
</li>
</ul>
<div class="arithmatex">\[\mathbf{J}(\mathbf{s}) =
\begin{bmatrix}
s_1 (1 - s_1) &amp; -s_1 s_2 &amp; \cdots &amp; -s_1 s_n \\
-s_2 s_1 &amp; s_2 (1 - s_2) &amp; \cdots &amp; -s_2 s_n \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
-s_n s_1 &amp; -s_n s_2 &amp; \cdots &amp; s_n (1 - s_n)
\end{bmatrix}\]</div>
<ul>
<li>Loss Functions: Basic loss functions like Mean Squared Error and Cross-Entropy to train simple models.</li>
</ul>
<h3 id="loss-functions">Loss Functions</h3>
<table>
<thead>
<tr>
<th>Loss Function</th>
<th>Type</th>
<th>When to Use</th>
<th>Advantages</th>
<th>Disadvantages</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Mean Squared Error (MSE)</strong></td>
<td>Regression</td>
<td>Regression with continuous targets</td>
<td>Simple, differentiable</td>
<td>Sensitive to outliers</td>
<td>✅</td>
</tr>
<tr>
<td><strong>Mean Absolute Error (MAE)</strong></td>
<td>Regression</td>
<td>Regression with noisy data</td>
<td>Less sensitive to outliers</td>
<td>Less smooth gradient</td>
<td>✅</td>
</tr>
<tr>
<td><strong>Cross-Entropy</strong></td>
<td>Classification</td>
<td>Binary or multi-class classification</td>
<td>Works well with probabilistic models</td>
<td>Sensitive to class imbalance</td>
<td>❌</td>
</tr>
<tr>
<td><strong>Hinge Loss (SVM)</strong></td>
<td>Classification</td>
<td>Support Vector Machines (SVM)</td>
<td>Efficient for margin classifiers</td>
<td>Not suitable for probabilistic tasks</td>
<td>❌</td>
</tr>
<tr>
<td><strong>Huber Loss</strong></td>
<td>Regression</td>
<td>Regression with outliers</td>
<td>Robust to outliers, smooth</td>
<td>Requires tuning of threshold <span class="arithmatex">\(\delta\)</span></td>
<td>❌</td>
</tr>
<tr>
<td><strong>KL Divergence</strong></td>
<td>Probabilistic Models</td>
<td>Variational inference, generative models</td>
<td>Compares probability distributions</td>
<td>Asymmetric, computationally expensive</td>
<td>❌</td>
</tr>
<tr>
<td><strong>NEGATIVE log likelyhood</strong></td>
<td>Classification</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>❌</td>
</tr>
</tbody>
</table>
<p><strong>i.    Mean Squared Error (MSE) Loss</strong></p>
<p>Formula:</p>
<p><span class="arithmatex">\(\text{MSE} = \frac{1}{2n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span></p>
<p>The derivative of MSE with respect to each prediction  <span class="arithmatex">\(\hat{y}_i\)</span>  is:</p>
<p><span class="arithmatex">\(\frac{\partial \text{MSE}}{\partial \hat{y}_i} = -\frac{1}{n}(y_i - \hat{y}_i)\)</span></p>
<p>Where:
-    <span class="arithmatex">\(y_i  = True value\)</span>
-    <span class="arithmatex">\(\hat{y}_i  = Predicted value\)</span>
-   <span class="arithmatex">\(n  = Number of data points\)</span></p>
<p>How it Works:</p>
<ul>
<li>MSE calculates the average of the squared differences between predicted and true values. It penalizes large errors more significantly due to the squaring of the difference.</li>
</ul>
<p><strong>ii.    Mean Absolute Error (MAE) Loss</strong></p>
<p>Formula:</p>
<p><span class="arithmatex">\(\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|\)</span></p>
<p>The derivative of MAE with respect to each prediction <span class="arithmatex">\(\hat{y}_i\)</span> is:</p>
<div class="arithmatex">\[
\frac{\partial \text{MAE}}{\partial \hat{y}_i} = 
\begin{cases}
-\frac{1}{n}, &amp; \text{if } \hat{y}_i &gt; y_i \\
\frac{1}{n}, &amp; \text{if } \hat{y}_i &lt; y_i
\end{cases}
\]</div>
<p>Where:
-    <span class="arithmatex">\(y_i  = True value\)</span>
-    <span class="arithmatex">\(\hat{y}_i  = Predicted value\)</span>
-   <span class="arithmatex">\(n  = Number of data points\)</span></p>
<p>Handling Non-Differentiability at <span class="arithmatex">\(y_i = \hat{y}_i\)</span>:</p>
<ul>
<li>At <span class="arithmatex">\(y_i = \hat{y}_i\)</span>, the derivative is undefined because the slope of the absolute value changes abruptly. In practice:<ul>
<li>For optimization algorithms, $0# or small gradient value is often used.</li>
<li>Some frameworks introduce smooth approximations to <span class="arithmatex">\(|x|\)</span> (e.g, Huber loss) to avoid the issue of non-differentiability.</li>
</ul>
</li>
</ul>
<p>How it Works:</p>
<ul>
<li>MAE computes the average of the absolute differences between predicted and true values. Unlike MSE, it does not square the differences, which makes it less sensitive to large errors.</li>
</ul>
<p><strong>iii.    Cross-Entropy Loss</strong></p>
<p>Formula (for Binary Classification):</p>
<p><span class="arithmatex">\(\text{Binary Cross-Entropy} = - \frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]\)</span></p>
<p>For Multi-Class Classification:</p>
<p><span class="arithmatex">\(\text{Categorical Cross-Entropy} = - \sum_{i=1}^{n} \sum_{c=1}^{C} y_{ic} \log(\hat{y}_{ic})\)</span></p>
<p>Where:
-    <span class="arithmatex">\(y_i  = True probability distribution\)</span>
-    <span class="arithmatex">\(\hat{y}_i  = Predicted probability distribution\)</span>
-    <span class="arithmatex">\(C  = Number of classes\)</span>
-    <span class="arithmatex">\(y_{ic}  = 1 if the instance belongs to class  c , else 0\)</span>
-    $\hat{y}_{ic}  = Predicted probability for class  c $</p>
<p>How it Works:</p>
<ul>
<li>Cross-entropy loss measures the difference between two probability distributions: the true label distribution and the predicted probability distribution. It is widely used in classification tasks.</li>
</ul>
<p><strong>iv.    Hinge Loss (SVM Loss)</strong></p>
<p><strong>Formula:</strong></p>
<p><span class="arithmatex">\(\text{Hinge Loss} = \sum_{i=1}^{n} \max(0, 1 - y_i \hat{y}_i)\)</span></p>
<p><strong>Where:</strong>
-    <span class="arithmatex">\(y_i  = True label (+1 or -1)\)</span>
-    <span class="arithmatex">\(\hat{y}_i  = Predicted score (not probability)\)</span></p>
<p><strong>How it Works:</strong></p>
<ul>
<li>Hinge loss is used in Support Vector Machines (SVM) and other classification tasks. It penalizes predictions that are on the wrong side of the decision boundary and doesn’t penalize correctly classified points as long as they are on the correct side of the margin.</li>
</ul>
<p><strong>v.    Huber Loss</strong></p>
<p><strong>Formula:</strong></p>
<div class="arithmatex">\[\text{Huber}(\delta) =
\begin{cases}
\frac{1}{2}(y_i - \hat{y}_i)^2, &amp; \text{for } |y_i - \hat{y}_i| \leq \delta \\
\delta |y_i - \hat{y}_i| - \frac{1}{2} \delta^2, &amp; \text{otherwise}
\end{cases}\]</div>
<p><strong>Where:</strong>
-    <span class="arithmatex">\(\delta\)</span>  is a threshold that determines the transition from quadratic to linear loss.</p>
<p><strong>How it Works:</strong></p>
<ul>
<li>Huber loss combines both MSE and MAE. It behaves like MSE for small errors (i.e., when the absolute error is less than  \delta ) and like MAE for large errors. This makes it less sensitive to outliers compared to MSE while still being differentiable.</li>
</ul>
<p><strong>vi.    Kullback-Leibler Divergence (KL Divergence)</strong></p>
<p><strong>Formula:</strong></p>
<p><span class="arithmatex">\(\text{KL Divergence} = \sum_{i=1}^{n} p_i \log\left(\frac{p_i}{q_i}\right)\)</span></p>
<p><strong>Where:</strong>
-    <span class="arithmatex">\(p_i  = True distribution (e.g., true labels)\)</span>
-    <span class="arithmatex">\(q_i  = Predicted distribution\)</span></p>
<p><strong>How it Works:</strong></p>
<ul>
<li>
<p><span class="arithmatex">\(KL\)</span> divergence measures the difference between two probability distributions. It is asymmetric, meaning  <span class="arithmatex">\(\text{KL}(p \parallel q) \neq \text{KL}(q \parallel p)\)</span> .</p>
</li>
<li>
<p>Optimization Algorithms</p>
</li>
<li>
<p>Gradient Descent: Implement vanilla gradient descent for updating weights.</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Task</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>step optimizer</td>
<td>❌</td>
</tr>
</tbody>
</table>
<ul>
<li>Extensions: Planned support for optimizations like Stochastic Gradient Descent (SGD) and other optimizers (e.g., Adam) as the library progresses.</li>
</ul>
<h3 id="optimizers">Optimizers</h3>
<table>
<thead>
<tr>
<th>Task</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>ADAM</td>
<td>❌</td>
</tr>
<tr>
<td>SGD</td>
<td>❌</td>
</tr>
<tr>
<td>RMS PROP</td>
<td>❌</td>
</tr>
<tr>
<td>ADADELTA</td>
<td>❌</td>
</tr>
<tr>
<td>ADAGRAD</td>
<td>❌</td>
</tr>
<tr>
<td>ADAMW</td>
<td>❌</td>
</tr>
</tbody>
</table>
<ol>
<li>
<p>Training and Evaluation Loop</p>
</li>
<li>
<p>Forward and Backward Passes: Execution of forward pass and automatic differentiation for backpropagation.</p>
</li>
<li>
<p>Metrics Tracking: Calculate and log accuracy or loss during training.</p>
</li>
<li>
<p>Progress Display: Basic progress bar for training epochs and mini-batches.</p>
</li>
</ol>
<h3 id="layers">Layers</h3>
<table>
<thead>
<tr>
<th>Task</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>SEQUENTIAL</td>
<td>❌</td>
</tr>
<tr>
<td>LINEAR</td>
<td>❌</td>
</tr>
<tr>
<td>DROPOUT</td>
<td>❌</td>
</tr>
<tr>
<td>CONV2D</td>
<td>❌</td>
</tr>
<tr>
<td>CONV3D</td>
<td>❌</td>
</tr>
<tr>
<td>MAXPOOL2D</td>
<td>❌</td>
</tr>
<tr>
<td>MAXPOOL3D</td>
<td>❌</td>
</tr>
</tbody>
</table>
<h1 id="future-milestones">Future Milestones</h1>
<ol>
<li>
<p>Additional Neural Network Layers</p>
</li>
<li>
<p>Convolutional Layers: Add convolution layers for basic image-processing tasks.</p>
</li>
<li>
<p>Pooling Layers: Max and average pooling layers for reducing spatial dimensions.</p>
</li>
<li>
<p>Expanded Tensor Operations</p>
</li>
<li>
<p>Broadcasting: Support for basic broadcasting to handle mismatched tensor shapes.</p>
</li>
<li>
<p>Advanced Math Operations: Include more operations (e.g., exponentiation, logarithms) for increased model complexity.</p>
</li>
<li>
<p>GPU/Hardware Support</p>
</li>
<li>
<p>OpenCL/CUDA Integration: Explore integration with OpenCL or CUDA to leverage GPUs for faster computation.</p>
</li>
<li>
<p>SIMD Optimizations: Use SIMD instructions for faster CPU-based tensor operations.</p>
</li>
<li>
<p>Serialization and Model Exporting</p>
</li>
<li>
<p>Model Saving and Loading: Save model weights and parameters for reproducibility and deployment.</p>
</li>
<li>
<p>ONNX Export: Basic support for ONNX format export, allowing compatibility with other deep learning frameworks.</p>
</li>
<li>
<p>Python Bindings</p>
</li>
<li>
<p>Python API: Create a minimal Python API for easy usage and debugging, making it accessible for Python-based experimentation.</p>
</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../assets/mathjax-config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>